---
layout: home
search_exclude: true
image: images/logo.png
---

![]({{site.baseurl}}/images/book_cover.jpg "https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/")

> "The preeminent book for the preeminent transformers library‚Äîa model of clarity!"
‚ÄîJeremy Howard, cofounder of fast.ai and professor at University of Queensland

> ‚ÄúA wonderfully clear and incisive guide to modern NLP's most essential library. Recommended!‚Äù
‚ÄîChristopher Manning, Thomas M. Siebel Professor in Machine Learning, Stanford University

<a class="btn btn-secondary" href="https://www.amazon.com/Natural-Language-Processing-Transformers-Applications-dp-1098103246/dp/1098103246/ref=mt_other?_encoding=UTF8&me=&qid=1643574211">Buy the book on Amazon</a>
<a class="btn btn-secondary" href="https://learning.oreilly.com/library/view/natural-language-processing/9781098103231/">Read the book online at O'Reilly</a>
<a class="btn btn-secondary" href="https://github.com/nlp-with-transformers/notebooks">Download the book's code</a>


Since their introduction in 2017, transformers have quickly become the dominant architecture for achieving state-of-the-art results on a variety of natural language processing tasks. If you're a data scientist or coder, this practical book shows you how to train and scale these large models using Hugging Face Transformers, a Python-based deep learning library.

Transformers have been used to write realistic news stories, improve Google Search queries, and even create chatbots that tell corny jokes. In this guide, authors [Lewis Tunstall](https://twitter.com/_lewtun), [Leandro von Werra](https://twitter.com/lvwerra), and [Thomas Wolf](https://twitter.com/Thom_Wolf) use a hands-on approach to teach you how transformers work and how to integrate them in your applications. You'll quickly learn a variety of tasks they can help you solve.

* Build, debug, and optimize transformer models for core NLP tasks, such as text classification, named entity recognition, and question answering
* Learn how transformers can be used for cross-lingual transfer learning
* Apply transformers in real-world scenarios where labeled data is scarce
* Make transformer models efficient for deployment using techniques such as distillation, pruning, and quantization
* Train transformers from scratch and learn how to scale to multiple GPUs and distributed environments

# News üóûÔ∏è

### January 31, 2022
Lewis will be joining [Abhishek Thakur](https://twitter.com/abhi1thakur) to talk about the book and various techniques you can use to optimize Transformer models for production environments. We'll also be giving away 5 copies of the book -- join the event [here](https://www.youtube.com/watch?v=5E0nlHWgMMU)!

# Contact

For questions, comments, or requests to interview the authors, please send an email to [contact@transformersbook.com](mailto:contact@transformersbook.com).

To submit errata or errors in the book, please do so via the [O'Reilly platform](https://www.oreilly.com/catalog/errata.csp?isbn=0636920512424).